{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangsmith\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangsmith\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import Client\n",
    "from langsmith import evaluate\n",
    "from langsmith.schemas import Example, Run\n",
    "from pydantic import BaseModel, Field\n",
    "import numpy as np\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"LANGCHAIN_API_KEY\"\n",
    "\n",
    "client = Client(api_key=os.environ[\"LANGCHAIN_API_KEY\"], api_url=os.environ[\"LANGCHAIN_ENDPOINT\"])\n",
    "examples = list(client.list_examples(dataset_name=\"Neurapolis ETL Sectionizer\"))\n",
    "\n",
    "def type_comparator(root_run: Run, example: Example) -> dict:\n",
    "    outputs = root_run.outputs\n",
    "    example_output = example.outputs\n",
    "    if outputs['output'].type == example_output['type']:\n",
    "        score = 1\n",
    "    else:\n",
    "        score = 0\n",
    "    return {\n",
    "        \"key\": \"type_comparison\",\n",
    "        \"score\": score,\n",
    "    }\n",
    "\n",
    "def calculate_splits_similarity(proposed_splits, golden_splits):\n",
    "    # Calculate total lines\n",
    "    total_lines = max(\n",
    "        max(split.line_number for split in proposed_splits),\n",
    "        max(split['line_number'] for split in golden_splits)\n",
    "    )\n",
    "    proposed = np.array([split.line_number for split in proposed_splits])\n",
    "    golden = np.array([split['line_number'] for split in golden_splits])\n",
    "    # Normalize the splits\n",
    "    proposed_norm = proposed / total_lines\n",
    "    golden_norm = golden / total_lines\n",
    "    # Calculate the minimum distance for each proposed split to any golden split\n",
    "    distances = np.min(np.abs(proposed_norm[:, np.newaxis] - golden_norm), axis=1)\n",
    "    # Calculate similarity score (inverse of distance)\n",
    "    similarities = 1 / (1 + distances)\n",
    "    return np.mean(similarities)\n",
    "\n",
    "def evaluate_splits(root_run: Run, example: Example) -> dict:\n",
    "    outputs = root_run.outputs['output']\n",
    "    example_output = example.outputs\n",
    "    if outputs.type != 'NOT_RELATED_TOPICS' or example_output['type'] != 'NOT_RELATED_TOPICS':\n",
    "        return {\n",
    "            \"key\": \"splits_comparison\",\n",
    "            \"score\": None,  # No score for non-NOT_RELATED_TOPICS types\n",
    "            \"metadata\": {\n",
    "                \"similarity\": \"N/A\",\n",
    "                \"reason\": \"Not applicable for this document type\"\n",
    "            }\n",
    "        }\n",
    "    proposed_splits = outputs.splits\n",
    "    golden_splits = example_output['splits']\n",
    "    # If either proposed or golden splits are empty, return a special score\n",
    "    if not proposed_splits or not golden_splits:\n",
    "        return {\n",
    "            \"key\": \"splits_comparison\",\n",
    "            \"score\": 0 if (proposed_splits and not golden_splits) or (not proposed_splits and golden_splits) else None,\n",
    "            \"metadata\": {\n",
    "                \"similarity\": \"N/A\",\n",
    "                \"reason\": \"Either proposed or golden splits are empty\"\n",
    "            }\n",
    "        }\n",
    "    similarity = calculate_splits_similarity(proposed_splits, golden_splits)\n",
    "    return {\n",
    "        \"key\": \"splits_comparison\",\n",
    "        \"score\": similarity,\n",
    "        \"metadata\": {\n",
    "            \"similarity\": similarity,\n",
    "            \"proposed_splits\": [split.line_number for split in proposed_splits],\n",
    "            \"golden_splits\": [split['line_number'] for split in golden_splits],\n",
    "        }\n",
    "    }\n",
    "\n",
    "def evaluate_split_count(root_run: Run, example: Example) -> dict:\n",
    "    outputs = root_run.outputs['output']\n",
    "    example_output = example.outputs\n",
    "    if outputs.type != 'NOT_RELATED_TOPICS' or example_output['type'] != 'NOT_RELATED_TOPICS':\n",
    "        return {\n",
    "            \"key\": \"split_count_comparison\",\n",
    "            \"score\": None,  # No score for non-NOT_RELATED_TOPICS types\n",
    "            \"metadata\": {\n",
    "                \"count_match\": \"N/A\",\n",
    "                \"reason\": \"Not applicable for this document type\"\n",
    "            }\n",
    "        }\n",
    "    proposed_splits = outputs.splits\n",
    "    golden_splits = example_output['splits']\n",
    "    proposed_count = len(proposed_splits)\n",
    "    golden_count = len(golden_splits)\n",
    "    # Calculate the difference in counts\n",
    "    count_difference = abs(proposed_count - golden_count)\n",
    "    # Normalize the score based on the difference\n",
    "    max_difference = max(proposed_count, golden_count)\n",
    "    score = 1 - (count_difference / max_difference) if max_difference > 0 else 1\n",
    "    return {\n",
    "        \"key\": \"split_count_comparison\",\n",
    "        \"score\": score,\n",
    "        \"metadata\": {\n",
    "            \"proposed_count\": proposed_count,\n",
    "            \"golden_count\": golden_count,\n",
    "            \"count_difference\": count_difference\n",
    "        }\n",
    "    }\n",
    "\n",
    "def compound_metric(root_run: Run, example: Example) -> dict:\n",
    "    type_score = type_comparator(root_run, example)['score']\n",
    "    splits_result = evaluate_splits(root_run, example)\n",
    "    count_result = evaluate_split_count(root_run, example)\n",
    "    splits_score = splits_result['score']\n",
    "    count_score = count_result['score']\n",
    "    # If the type is not NOT_RELATED_TOPICS, only consider the type score\n",
    "    if splits_score is None or count_score is None:\n",
    "        compound_score = type_score\n",
    "        reason = \"Only type comparison applicable\"\n",
    "    else:\n",
    "        # Combine scores (you can adjust the weights as needed)\n",
    "        compound_score = 0.4 * type_score + 0.3 * splits_score + 0.3 * count_score\n",
    "        reason = \"All metrics applicable\"\n",
    "    return {\n",
    "        \"key\": \"compound_metric\",\n",
    "        \"score\": compound_score,\n",
    "        \"metadata\": {\n",
    "            \"type_score\": type_score,\n",
    "            \"splits_score\": splits_score,\n",
    "            \"count_score\": count_score,\n",
    "            \"reason\": reason\n",
    "        }\n",
    "    }\n",
    "\n",
    "class FileSectionizerLineSplitLlmDataModel(BaseModel):\n",
    "    line_number: int = Field(\n",
    "        description=\"Zeilennummer, an der das Dokument aufgeteilt werden soll\"\n",
    "    )\n",
    "    reason: str = Field(\n",
    "        description=\"Kurze Begründung für die Aufteilung an dieser Stelle\"\n",
    "    )\n",
    "\n",
    "class FileSectionizerLlmDataModel(BaseModel):\n",
    "    type: str = Field(description=\"Der Inhaltstyp des Dokuments\")\n",
    "    reason: str = Field(\n",
    "        description=\"Kurze Begründung für die Entscheidung des Inhaltstyps\"\n",
    "    )\n",
    "    splits: list[FileSectionizerLineSplitLlmDataModel] = Field(\n",
    "        description=\"Liste der Aufteilungen\",\n",
    "        default=[],\n",
    "    )\n",
    "\n",
    "prompt_template_string = \"\"\"Du bist der Datei-Sektionierer im Rats Informations System (RIS) für Freiburg. Deine Hauptaufgabe ist die präzise Klassifizierung und, wenn nötig, Segmentierung von Dokumenten.\n",
    "Dokumenttypen:\n",
    "\n",
    "NOT_RELATED_TOPICS: Dokumente mit mehreren klar unverbundenen Themen, typischerweise Sitzungsprotokolle oder Ergebnismitteilungen mit verschiedenen Tagesordnungspunkten (TOPs).\n",
    "RELATED_TOPICS: Dokumente, die sich auf ein Hauptthema konzentrieren, auch wenn sie Unterthemen oder detaillierte Diskussionen enthalten. Beispiele sind Bekanntmachungen, detaillierte Berichte zu einem spezifischen Thema oder Beschlussvorlagen zu einem einzelnen Thema.\n",
    "OTHER: Strukturierte Dokumente wie Einladungen zu Sitzungen, Tagesordnungen oder ähnliche Formate, die nicht in die anderen Kategorien passen.\n",
    "\n",
    "Analyserichtlinien:\n",
    "\n",
    "Untersuche das gesamte Dokument sorgfältig, einschließlich Kopfzeilen, Datum, Absender und Empfänger.\n",
    "Achte besonders auf die Struktur des Dokuments, Überschriften, Tagesordnungspunkte (TOPs) und thematische Abschnitte.\n",
    "Berücksichtige den Gesamtkontext und -zweck des Dokuments.\n",
    "\n",
    "Klassifizierung:\n",
    "\n",
    "Wähle den am besten passenden Dokumenttyp basierend auf der Gesamtstruktur und dem Inhalt.\n",
    "Gib eine detaillierte Begründung für deine Entscheidung an, die spezifische Elemente des Dokuments erwähnt.\n",
    "\n",
    "Segmentierung:\n",
    "\n",
    "Segmentiere NUR Dokumente des Typs NOT_RELATED_TOPICS.\n",
    "Markiere den Beginn jeder neuen Sektion mit der entsprechenden Zeilennummer.\n",
    "Segmentiere bei klaren Themenübergängen, insbesondere bei neuen Tagesordnungspunkten (TOPs).\n",
    "Für OTHER und RELATED_TOPICS ist keine Segmentierung erforderlich.\n",
    "\n",
    "Wichtige Hinweise:\n",
    "\n",
    "Klassifiziere Einladungen zu Sitzungen, Tagesordnungen und ähnliche strukturierte Dokumente immer als OTHER, unabhängig von der Anzahl der aufgelisteten Themen.\n",
    "Ergebnismitteilungen und Sitzungsprotokolle mit mehreren TOPs sind in der Regel als NOT_RELATED_TOPICS zu klassifizieren und zu segmentieren.\n",
    "Bei RELATED_TOPICS ist keine Segmentierung erforderlich, auch wenn Unterthemen vorhanden sind.\n",
    "Beachte, dass die ersten Zeilen oft Metadaten wie Datum, Absender oder Empfänger enthalten und nicht als separate Segmente behandelt werden sollten.\n",
    "Sei konsistent in deiner Analyse und Segmentierung über verschiedene Dokumenttypen hinweg.\n",
    "Berücksichtige, dass manche Dokumente mit einer Überschrift oder einem Titel beginnen können, der nicht als separates Segment gezählt werden sollte.\n",
    "Beschlussvorlagen oder detaillierte Berichte zu einem einzelnen Hauptthema sollten als RELATED_TOPICS klassifiziert werden, auch wenn sie verschiedene Aspekte dieses Themas diskutieren.\n",
    "\n",
    "<Inhalt des Dokuments>\n",
    "{text}\n",
    "</Inhalt des Dokuments>\n",
    "Bitte klassifiziere das Dokument und, falls erforderlich, segmentiere es entsprechend den oben genannten Richtlinien. Gib eine detaillierte Begründung für deine Klassifizierung an und erkläre, warum du dich für oder gegen eine Segmentierung entschieden hast.\"\"\"\n",
    "prompt_template = PromptTemplate.from_template(prompt_template_string)\n",
    "chain = prompt_template | AzureChatOpenAI(deployment_name=\"gpt-4o\").with_structured_output(FileSectionizerLlmDataModel)\n",
    "result = evaluate(\n",
    "    lambda x: chain.invoke(x[\"text_lines\"]),\n",
    "    data=examples,\n",
    "    evaluators=[type_comparator, evaluate_splits, evaluate_split_count, compound_metric],\n",
    "    experiment_prefix=\"example\",\n",
    "    client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average of the compound metric\n",
    "compound_metric_scores = []\n",
    "\n",
    "for item in result:\n",
    "    evaluation_results = item.get('evaluation_results', {}).get('results', [])\n",
    "    for eval_result in evaluation_results:\n",
    "        if eval_result.key == 'compound_metric':\n",
    "            compound_metric_scores.append(eval_result.score)\n",
    "\n",
    "average_compound_metric = sum(compound_metric_scores) / len(compound_metric_scores) if compound_metric_scores else 0\n",
    "\n",
    "print(f\"Average Compound Metric: {average_compound_metric:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of example outputs to actual outputs\n",
    "output_mapping = {}\n",
    "\n",
    "for result in result:\n",
    "    run = result['run']\n",
    "    example = result['example']\n",
    "    \n",
    "    example_id = str(example.id)\n",
    "    \n",
    "    # Map the example output to the actual output\n",
    "    output_mapping[example_id] = {\n",
    "        'example_output': example.outputs,\n",
    "        'actual_output': run.outputs['output']\n",
    "    }\n",
    "\n",
    "# Print the mapping for verification\n",
    "for example_id, outputs in output_mapping.items():\n",
    "    print(f\"Example ID: {example_id}\")\n",
    "    print(f\"Example Output: {outputs['example_output']}\")\n",
    "    print(f\"Actual Output: {outputs['actual_output']}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_deviations(results):\n",
    "    deviations = []\n",
    "    for item in results:\n",
    "        run = item.get('run')\n",
    "        example = item.get('example')\n",
    "        \n",
    "        if not run or not example:\n",
    "            continue\n",
    "        \n",
    "        example_output = example.outputs\n",
    "        actual_output = run.outputs.get('output')\n",
    "        \n",
    "        if not actual_output:\n",
    "            continue\n",
    "        \n",
    "        # Check for deviations in 'type' and 'splits'\n",
    "        if isinstance(actual_output, FileSectionizerLlmDataModel):\n",
    "            type_mismatch = actual_output.type != example_output.get('type')\n",
    "            splits_mismatch = actual_output.splits != example_output.get('splits')\n",
    "        else:\n",
    "            type_mismatch = getattr(actual_output, 'type', None) != example_output.get('type')\n",
    "            splits_mismatch = getattr(actual_output, 'splits', None) != example_output.get('splits')\n",
    "        \n",
    "        if type_mismatch or False:\n",
    "            deviations.append({\n",
    "                'example_id': str(example.id),\n",
    "                'example_output': example_output,\n",
    "                'actual_output': actual_output,\n",
    "                'type_mismatch': type_mismatch,\n",
    "                'splits_mismatch': splits_mismatch\n",
    "            })\n",
    "    return deviations\n",
    "\n",
    "# Analyze deviations\n",
    "try:\n",
    "    deviations = analyze_deviations(list(result))\n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing deviations: {str(e)}\")\n",
    "    deviations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_updated_prompt(results, current_prompt):\n",
    "    analysis = []\n",
    "    for item in results:\n",
    "        example = item.get('example')\n",
    "        run = item.get('run')\n",
    "        if not example or not run:\n",
    "            continue\n",
    "        \n",
    "        example_output = example.outputs\n",
    "        actual_output = run.outputs.get('output')\n",
    "        if not actual_output:\n",
    "            continue\n",
    "        \n",
    "        type_mismatch = actual_output.type != example_output.get('type')\n",
    "        splits_mismatch = actual_output.splits != example_output.get('splits')\n",
    "        \n",
    "        analysis.append({\n",
    "            'example_id': str(example.id),\n",
    "            'expected_type': example_output.get('type'),\n",
    "            'actual_type': actual_output.type,\n",
    "            'type_mismatch': type_mismatch,\n",
    "            'splits_mismatch': splits_mismatch,\n",
    "            'example_input': example.inputs.get('text_lines'),\n",
    "            'example_output': example_output,\n",
    "            'actual_output': actual_output\n",
    "        })\n",
    "    \n",
    "    descriptions = \"\\n\\n\".join([\n",
    "        f\"ID: {a['example_id']}\\n\"\n",
    "        f\"Input: {a['example_input'][:200]}...\\n\"\n",
    "        f\"Expected: {a['expected_type']}\\n\"\n",
    "        f\"Actual: {a['actual_type']}\\n\"\n",
    "        f\"Type Mismatch: {a['type_mismatch']}\\n\"\n",
    "        f\"Splits Mismatch: {a['splits_mismatch']}\\n\"\n",
    "        f\"Expected Output: {a['example_output']}\\n\"\n",
    "        f\"Actual Output: {a['actual_output']}\\n\"\n",
    "        for a in analysis if a['type_mismatch'] or a['splits_mismatch']\n",
    "    ])\n",
    "    \n",
    "    update_prompt = f\"\"\"\n",
    "Analyze the following examples and improve the prompt to enhance document categorization accuracy:\n",
    "\n",
    "{descriptions}\n",
    "\n",
    "Current Prompt:\n",
    "{current_prompt}\n",
    "\n",
    "Provide only the updated prompt:\n",
    "\"\"\"\n",
    "    model = AzureChatOpenAI(deployment_name=\"gpt-4o\")\n",
    "    updated_prompt = model.invoke(update_prompt)\n",
    "    \n",
    "    summary_prompt = f\"\"\"\n",
    "Summarize key changes between:\n",
    "\n",
    "Original:\n",
    "{current_prompt}\n",
    "\n",
    "Updated:\n",
    "{updated_prompt.content}\n",
    "\"\"\"\n",
    "    summary = model.invoke(summary_prompt)\n",
    "    \n",
    "    print(\"\\033[94mChange Summary:\\033[0m\")\n",
    "    print(\"\\033[94m\" + summary.content + \"\\033[0m\")\n",
    "    \n",
    "    print(\"\\033[92mUpdated Prompt:\\033[0m\")\n",
    "    print(\"\\033[92m\" + updated_prompt.content + \"\\033[0m\")\n",
    "    \n",
    "    return updated_prompt.content\n",
    "\n",
    "current_prompt = prompt_template_string\n",
    "updated_prompt = generate_updated_prompt(deviations, current_prompt)\n",
    "print(\"Updated Prompt:\")\n",
    "print(updated_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below leads refinement as after some rounds of iteration it throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   File \"/var/folders/_x/18_1sczx5634lnky__ljb3900000gn/T/ipykernel_13744/2123409029.py\", line 6, in evaluate_split_count\n",
    "#     if outputs.type != 'NOT_RELATED_TOPICS' or example_output['type'] != 'NOT_RELATED_TOPICS':\n",
    "#        ^^^^^^^^^^^^\n",
    "# AttributeError: 'NoneType' object has no attribute 'type'\n",
    "# Error running evaluator <DynamicRunEvaluator compound_metric> on run 45bbb681-d7b6-4e2e-8587-f21e50ee5374: AttributeError(\"'NoneType' object has no attribute 'type'\")\n",
    "# Traceback (most recent call last):\n",
    "#   File \"/Users/pascal/neurapolis/evals/.venv/lib/python3.12/site-packages/langsmith/evaluation/_runner.py\", line 1344, in _run_evaluators\n",
    "#     evaluator_response = evaluator.evaluate_run(\n",
    "#                          ^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#   File \"/Users/pascal/neurapolis/evals/.venv/lib/python3.12/site-packages/langsmith/evaluation/evaluator.py\", line 327, in evaluate_run\n",
    "#     result = self.func(\n",
    "#              ^^^^^^^^^^\n",
    "#   File \"/Users/pascal/neurapolis/evals/.venv/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 646, in wrapper\n",
    "#     raise e\n",
    "#   File \"/Users/pascal/neurapolis/evals/.venv/lib/python3.12/site-packages/langsmith/run_helpers.py\", line 643, in wrapper\n",
    "#     function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
    "#                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#   File \"/var/folders/_x/18_1sczx5634lnky__ljb3900000gn/T/ipykernel_13744/3569692088.py\", line 2, in compound_metric\n",
    "#     type_score = type_comparator(root_run, example)['score']\n",
    "#                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#   File \"/var/folders/_x/18_1sczx5634lnky__ljb3900000gn/T/ipykernel_13744/4164126165.py\", line 32, in type_comparator\n",
    "#     if outputs['output'].type == example_output['type']:\n",
    "#        ^^^^^^^^^^^^^^^^^^^^^^\n",
    "# AttributeError: 'NoneType' object has no attribute 'type'\n",
    "# 55it [01:03,  1.15s/it]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_template = prompt_template_string\n",
    "version = 1\n",
    "desired_threshold = 0.95  # Set your desired compound metric threshold\n",
    "\n",
    "while True:\n",
    "    # Instantiate the prompt and LLM\n",
    "    prompt_template = PromptTemplate.from_template(current_template)\n",
    "    chain = prompt_template | AzureChatOpenAI(deployment_name=\"gpt-4o\").with_structured_output(FileSectionizerLlmDataModel)\n",
    "    \n",
    "    # Run evaluations\n",
    "    result = evaluate(\n",
    "        lambda x: chain.invoke(x[\"text_lines\"]),\n",
    "        data=examples,\n",
    "        evaluators=[type_comparator, evaluate_splits, evaluate_split_count, compound_metric],\n",
    "        experiment_prefix=\"example\",\n",
    "        metadata={\n",
    "            \"version\": f\"1.0.{version}\",\n",
    "            \"prompt_revision\": f\"revision_{version}\"\n",
    "        },\n",
    "        client=client\n",
    "    )\n",
    "    \n",
    "    # Calculate the average compound metric\n",
    "    compound_metric_scores = []\n",
    "    for item in result:\n",
    "        evaluation_results = item.get('evaluation_results', {}).get('results', [])\n",
    "        for eval_result in evaluation_results:\n",
    "            if eval_result.key == 'compound_metric':\n",
    "                compound_metric_scores.append(eval_result.score)\n",
    "    average_compound_metric = sum(compound_metric_scores) / len(compound_metric_scores) if compound_metric_scores else 0\n",
    "    print(f\"Average Compound Metric (Version {version}): {average_compound_metric:.4f}\")\n",
    "    \n",
    "    # Check if desired threshold is met\n",
    "    if average_compound_metric >= desired_threshold:\n",
    "        print(\"Desired performance achieved.\")\n",
    "        break\n",
    "    \n",
    "    # Analyze deviations\n",
    "    deviations = analyze_deviations(result)\n",
    "    \n",
    "    # If no deviations, break the loop\n",
    "    if not deviations:\n",
    "        print(\"No deviations found. Model performance is optimal.\")\n",
    "        break\n",
    "    \n",
    "    # Generate updated prompt automatically\n",
    "    updated_prompt = generate_updated_prompt(deviations, current_template)\n",
    "    \n",
    "    # Update the current template with the new prompt\n",
    "    current_template = updated_prompt\n",
    "    \n",
    "    version += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
