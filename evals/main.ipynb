{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langsmith import Client, traceable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client()\n",
    "\n",
    "# Set up OpenAI API key (ensure this is set in your environment variables)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
    "\n",
    "# Define the evaluation target\n",
    "@traceable\n",
    "def label_text(text):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "    chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    result = chat_model.invoke(messages)\n",
    "    return result.content\n",
    "\n",
    "# Create a dataset\n",
    "examples = [\n",
    "    (\"Shut up, idiot\", \"Toxic\"),\n",
    "    (\"You're a wonderful person\", \"Not toxic\"),\n",
    "    (\"This is the worst thing ever\", \"Toxic\"),\n",
    "    (\"I had a great day today\", \"Not toxic\"),\n",
    "    (\"Nobody likes you\", \"Toxic\"),\n",
    "    (\"This is unacceptable. I want to speak to the manager.\", \"Not toxic\"),\n",
    "]\n",
    "\n",
    "dataset_name = \"Toxic Queries\"\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "inputs, outputs = zip(*[({\"text\": text}, {\"label\": label}) for text, label in examples])\n",
    "client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)\n",
    "\n",
    "# Define the evaluator\n",
    "def correct_label(run, example):\n",
    "    score = run.outputs.get(\"output\") == example.outputs.get(\"label\")\n",
    "    return {\"score\": int(score), \"key\": \"correct_label\"}\n",
    "\n",
    "# Run the evaluation\n",
    "results = evaluate(\n",
    "    lambda inputs: label_text(inputs[\"text\"]),\n",
    "    data=dataset_name,\n",
    "    evaluators=[correct_label],\n",
    "    experiment_prefix=\"Toxic Queries\",\n",
    "    description=\"Testing the baseline system.\",\n",
    ")\n",
    "\n",
    "print(f\"Evaluation results: {results}\")\n",
    "\n",
    "# Example of using a LangChain runnable\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\"),\n",
    "    (\"user\", \"{text}\")\n",
    "])\n",
    "chat_model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | chat_model | output_parser\n",
    "\n",
    "# Evaluate the LangChain runnable\n",
    "langchain_results = evaluate(\n",
    "    chain.invoke,\n",
    "    data=dataset_name,\n",
    "    evaluators=[correct_label],\n",
    "    experiment_prefix=\"Toxic Queries LangChain\",\n",
    ")\n",
    "\n",
    "print(f\"LangChain evaluation results: {langchain_results}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
